labs(title = "Predicted vs Actual Values",
x = "Actual log_Sale_Price",
y = "Predicted log_Sale_Price") +
theme_minimal()
#defining model
model_lasso =
linear_reg(penalty = tune(), mixture = 1) |>
set_engine("glmnet")
#setting lambdas
#Values range from 10^-7 because from iterative process it seems that lower alphas perform better
lambdas = 10 ^ seq(from = -7, to = 0, length = 1e3)
#creating v-fold cross-validation
housing_cv = housing_train |> vfold_cv(v = 5)
#defining workflow
workflow_lasso = workflow() |>
add_model(model_lasso) |>
add_recipe(housing_recipe)
#tuning for a Lasso regression model
lasso_cv = workflow_lasso |>
tune_grid(
housing_cv,
grid = data.frame(penalty = lambdas),
metrics = metric_set(yardstick::rmse)
)
#get the best lasso
lasso_cv |> show_best()
#the best parameter
final_lasso =
workflow_lasso |>
finalize_workflow(select_best(lasso_cv, metric = "rmse"))
final_lasso
#get the best lasso
lasso_cv |> show_best()
#the best parameter
final_lasso =
workflow_lasso |>
finalize_workflow(select_best(lasso_cv, metric = "rmse"))
final_lasso
library(ggplot2)
all_lasso=lasso_cv |> show_best(n=1000) #get all models
#plotting
library(ggplot2)
#plotting standard error which is variance
ggplot(all_lasso, aes(x = penalty, y=std_err)) +
geom_point(color = "blue", size = 1)
#plotting mean rmse which is bias
ggplot(all_lasso, aes(x = penalty)) +
geom_point(aes(y = mean), color = "pink", size = 1)
autoplot(lasso_cv, metric="rmse")
#seeing how the best parameter performed in test data
final_fit_lasso = final_lasso |> fit(data=housing_train) |> last_fit(split=housing_split)
#getting the coefficients from best models
coefs = final_fit_lasso |>
extract_fit_parsnip() |>
tidy()
#getting non-zero coefs
coefs_nonzero = coefs |>
filter(estimate > 0) |>
arrange(desc(estimate))
#creating lists of good and not good predictors
coef_lasso = coefs_nonzero$term
all_coefs=coefs$term
coef_lasso <- coef_lasso[coef_lasso != "(Intercept)"]
coefs_nonzero
print(setdiff(all_coefs, coef_lasso))
final_fit_lasso |> collect_metrics()
library(recipes)
library(parsnip)
housing_recipe1 =
# Define the recipe: Rating predicted by all other vars in housing_train
recipe(log_Sale_Price ~ ., data = housing_train) |>
# Create log terms for numeric predictors
#step_log(Lot_Area, First_Flr_SF, Gr_Liv_Area) |>
# Normalize log modified vars
step_normalize(all_numeric_predictors()) |>
step_poly(all_numeric_predictors(), degree=2) |>
#Dummies for nominal vars
step_dummy(all_nominal_predictors()) |>
#update_role(all_predictors(), roles = (predictor = coef_lasso)) |>
#step_select(all_numeric_predictors()) |>
# Remove predictors with near-zero variance (improves stability)
step_nzv(all_numeric_predictors()) |>
# Remove very collinear vars
step_corr(all_numeric_predictors(), method = "spearman",threshold = 0.9)
#getting cleaned data
system.time({
housing_clean <- housing_recipe1 |> prep() |> juice()
})
model_ridge =
linear_reg(penalty = tune(), mixture = 0) |>
set_engine("glmnet")
#setting lambdas
lambdas = 10 ^ seq(from = -5, to = 2, length = 1e3)
#creating v-fold cross-validation
housing_cv = housing_train |> vfold_cv(v = 5)
#defining workflow
workflow_ridge = workflow() |>
add_model(model_ridge) |>
add_recipe(housing_recipe1)
#tuning for a Ridge regression model
ridge_cv = workflow_ridge |>
tune_grid(
housing_cv,
grid = data.frame(penalty = lambdas),
metrics = metric_set(yardstick::rmse)
)
#plotting rmse against alpha
autoplot(ridge_cv, metric = "rmse")
#looking at best 5 models
ridge_cv |> show_best()
#saving best ridge
final_ridge =
workflow_ridge |>
finalize_workflow(select_best(lasso_cv, metric = "rmse"))
#define the elasticnet model
model_net = linear_reg(penalty = tune(), mixture = tune()) |>
set_engine("glmnet")
#define the workflow
workflow_net = workflow() |>
add_recipe(housing_recipe1) |>
add_model(model_net)
#it chooses parameters itself
cv_net =
workflow_net |>
tune_grid(
housing_cv,
grid = grid_regular(mixture(), penalty(), levels=5:5),
metrics = metric_set(rmse)
)
#plotting
autoplot(cv_net, metric = "rmse")
#define the elasticnet model
model_net = linear_reg(penalty = tune(), mixture = tune()) |>
set_engine("glmnet")
#define the workflow
workflow_net = workflow() |>
add_recipe(housing_recipe1) |>
add_model(model_net)
#it chooses parameters itself
cv_net =
workflow_net |>
tune_grid(
housing_cv,
grid = grid_regular(mixture(), penalty(), levels=5:5),
metrics = metric_set(rmse)
)
#plotting
autoplot(cv_net, metric = "rmse")
#looking at best 5 models
cv_net |> show_best()
#saving best ridge
final_net =
workflow_net |>
finalize_workflow(select_best(cv_net, metric = "rmse"))
# Define a simple KNN model
model_knn = nearest_neighbor(neighbors = tune(), mode = "regression") %>%
set_engine("kknn", scale = TRUE)
#define the workflow
workflow_knn = workflow() |>
add_recipe(housing_recipe1) |>
add_model(model_knn)
#tuning
knn_cv =
workflow_knn |>
tune_grid(
resamples = housing_cv,
grid = data.frame(neighbors = seq(5, 20)),# Example grid for tuning k
metrics = metric_set(rmse)
)
#looking at best 5 models
knn_cv |> show_best()
#plotting
autoplot(knn_cv, metrics="rmse")
#saving best KNN
final_knn =
workflow_knn |>
finalize_workflow(select_best(knn_cv, metric = "rmse"))
#defining workflow
workflow_lasso = workflow() |>
add_model(model_lasso) |>
add_recipe(housing_recipe1)
#tuning for a Lasso regression model
lasso_cv = workflow_lasso |>
tune_grid(
housing_cv,
grid = data.frame(penalty = lambdas),
metrics = metric_set(yardstick::rmse)
)
#get the best lasso
lasso_cv |> show_best()
#the best parameter
final_lasso =
workflow_lasso |>
finalize_workflow(select_best(lasso_cv, metric = "rmse"))
final_lasso
#defining workflow
workflow_lasso = workflow() |>
add_model(model_lasso) |>
add_recipe(housing_recipe1)
#tuning for a Lasso regression model
lasso_cv = workflow_lasso |>
tune_grid(
housing_cv,
grid = data.frame(penalty = lambdas),
metrics = metric_set(yardstick::rmse)
)
#get the best lasso
lasso_cv |> show_best()
#the best parameter
final_lasso =
workflow_lasso |>
finalize_workflow(select_best(lasso_cv, metric = "rmse"))
final_lasso
final_fit_lasso |> collect_metrics()
#Strategy 2: Do Lasso for all
# Libraries
library(pacman)
p_load(tidyverse, tidymodels, skimr, glmnet, kknn)
library(tidymodels)
library(openxlsx)
# Data
merge_final <- read.csv("Data/Merging/merge_last.csv")
setwd("C:/Users/aitku/OneDrive/Рабочий стол/Fall 2023/Advanced_Data_Analysis/GitHub/ECNS560.TermProject.Ethanol/R")
setwd("C:/Users/aitku/OneDrive/Рабочий стол/Fall 2023/Advanced_Data_Analysis/GitHub/ECNS560.TermProject.Ethanol/")
#Strategy 2: Do Lasso for all
# Libraries
library(pacman)
p_load(tidyverse, tidymodels, skimr, glmnet, kknn)
library(tidymodels)
library(openxlsx)
# Data
merge_final <- read.csv("Data/Merging/merge_last.csv")
#splitting data
merge_final_model <- merge_final %>%
select(-X.1, -X, -state_abb, -missing_eth, -missing_corn_prod, -missing_corn_prices, -X.2, -ratio_e85)
merge_final_model$year <- as.character(merge_final_model$year)
set.seed(42)
e85_split = merge_final_model |> group_initial_split(group=state, prop = 0.8)
e85_train = e85_split |> training()
e85_test  = e85_split |> testing()
e85_split
#recipe
e85_recipe =
# Define the recipe: Rating predicted by all other vars in housing_train
recipe(e85 ~ ., data = e85_train) |>
update_role(state, new_role = "ID") |>
# Create log terms for numeric predictors
step_log(total, population) |>
# Normalize vars
step_normalize(all_numeric_predictors()) |>
#dummy for years
step_dummy(year) |>
#interactions
step_interact(~all_predictors():all_predictors()) |>
# Remove predictors with near-zero variance (improves stability)
step_zv(all_numeric_predictors()) |>
# Remove very collinear vars
step_corr(all_numeric_predictors(), method = "spearman",threshold = 0.9)
system.time({
e85_clean <- e85_recipe |> prep() |> juice()
})
#model
model_lasso =
linear_reg(penalty = tune(), mixture = 1) |>
set_engine("glmnet")
#setting lambdas
#Values range from 10^-7 because from iterative process it seems that lower alphas perform better
lambdas = 10 ^ seq(from = -3, to = 3, length = 1e3)
e85_cv = e85_train |> group_vfold_cv(group=state, v = 5)
#defining workflow
workflow_lasso = workflow() |>
add_model(model_lasso) |>
add_recipe(e85_recipe)
#tuning for a Lasso regression model
lasso_cv = workflow_lasso |>
tune_grid(
e85_cv,
grid = data.frame(penalty = lambdas),
metrics = metric_set(yardstick::rmse)
)
lasso_cv |> show_best()
final_lasso =
workflow_lasso |>
finalize_workflow(select_best(lasso_cv, metric = "rmse"))
final_lasso
#graph
autoplot(lasso_cv, metric="rmse")
#looking at coefficients
final_fit_lasso = final_lasso |> fit(data=e85_train) |> last_fit(split=e85_split)
coefs = final_fit_lasso |>
extract_fit_parsnip() |>
tidy()
#getting non-zero coefs
coefs_nonzero = coefs |>
filter(estimate > 0) |>
arrange(desc(estimate))
#graph
# Predicting values on the test set
lasso_fit <- final_lasso %>%
fit(data = e85_test)
predicted_values <- predict(lasso_fit, new_data = e85_test)
# Plotting predicted vs actual values
library(ggplot2)
predicted_values$actuals=e85_test$e85
plot1=ggplot(data=predicted_values, aes(x = actuals, y = .pred)) +
geom_point() +
geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
labs(title = "Predicted vs Actual Values",
x = "Actual #E85 stations",
y = "Predicted #E85 stations") +
theme_minimal()
ggsave("Outputs/Econometrics_Analysis/predicted_actual.png", plot1, width = 8, height = 6, units = "in", dpi = 300)
#saving non-zero coefs
library(openxlsx)
write.xlsx(coefs_nonzero, file = "Outputs/Econometrics_Analysis/coefs_nonzero.xlsx", rowNames = FALSE)
View(coefs_nonzero)
finalize_workflow(select_best(lasso_cv, metric = "rmse"))
final_lasso
final_fit_net = final_lasso |> fit(data=e85_train) |> last_fit(split=e85_split)
final_fit_net |> collect_metrics()
final_fit_lasso = final_lasso |> fit(data=e85_train) |> last_fit(split=e85_split)
final_fit_lasso |> collect_metrics()
setwd("C:/Users/aitku/OneDrive/Рабочий стол/Fall 2023/Advanced_Data_Analysis/GitHub/ECNS560.TermProject.Ethanol/")
#Strategy 2: Do Lasso for all
# Libraries
library(pacman)
p_load(tidyverse, tidymodels, skimr, glmnet, kknn)
library(tidymodels)
library(openxlsx)
# Data
merge_final <- read.csv("Data/Merging/merge_last.csv")
#splitting data
merge_final_model <- merge_final %>%
select(-X.1, -X, -state_abb, -missing_eth, -missing_corn_prod, -missing_corn_prices, -X.2, -ratio_e85)
merge_final_model$year <- as.character(merge_final_model$year)
set.seed(42)
e85_split = merge_final_model |> group_initial_split(group=state, prop = 0.8)
e85_train = e85_split |> training()
e85_test  = e85_split |> testing()
e85_split
#recipe
e85_recipe =
# Define the recipe: Rating predicted by all other vars in housing_train
recipe(e85 ~ ., data = e85_train) |>
update_role(state, new_role = "ID") |>
# Create log terms for numeric predictors
step_log(total, population) |>
# Normalize vars
step_normalize(all_numeric_predictors()) |>
#dummy for years
step_dummy(year) |>
#interactions
step_interact(~all_predictors():all_predictors()) |>
# Remove predictors with near-zero variance (improves stability)
step_zv(all_numeric_predictors()) |>
# Remove very collinear vars
step_corr(all_numeric_predictors(), method = "spearman",threshold = 0.9)
system.time({
e85_clean <- e85_recipe |> prep() |> juice()
})
#model
model_lasso =
linear_reg(penalty = tune(), mixture = 1) |>
set_engine("glmnet")
#setting lambdas
#Values range from 10^-7 because from iterative process it seems that lower alphas perform better
lambdas = 10 ^ seq(from = -3, to = 3, length = 1e3)
e85_cv = e85_train |> group_vfold_cv(group=state, v = 5)
#defining workflow
workflow_lasso = workflow() |>
add_model(model_lasso) |>
add_recipe(e85_recipe)
#tuning for a Lasso regression model
lasso_cv = workflow_lasso |>
tune_grid(
e85_cv,
grid = data.frame(penalty = lambdas),
metrics = metric_set(yardstick::rmse)
)
lasso_cv |> show_best()
final_lasso =
workflow_lasso |>
finalize_workflow(select_best(lasso_cv, metric = "rmse"))
final_lasso
#graph
autoplot(lasso_cv, metric="rmse")
#on test data
final_fit_lasso = final_lasso |> fit(data=e85_train) |> last_fit(split=e85_split)
metrics= final_fit_lasso |> collect_metrics()
#looking at coefficients
coefs = final_fit_lasso |>
extract_fit_parsnip() |>
tidy()
#getting non-zero coefs
coefs_nonzero = coefs |>
filter(estimate > 0) |>
arrange(desc(estimate))
#graph
# Predicting values on the test set
lasso_fit <- final_lasso %>%
fit(data = e85_test)
predicted_values <- predict(lasso_fit, new_data = e85_test)
# Plotting predicted vs actual values
library(ggplot2)
predicted_values$actuals=e85_test$e85
plot1=ggplot(data=predicted_values, aes(x = actuals, y = .pred)) +
geom_point() +
geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
labs(title = "Predicted vs Actual Values",
x = "Actual #E85 stations",
y = "Predicted #E85 stations") +
theme_minimal()
ggsave("Outputs/Econometrics_Analysis/predicted_actual.png", plot1, width = 8, height = 6, units = "in", dpi = 300)
#saving non-zero coefs
library(openxlsx)
write.xlsx(coefs_nonzero, file = "Outputs/Econometrics_Analysis/coefs_nonzero.xlsx", rowNames = FALSE)
View(final_lasso)
View(metrics)
final_lasso
#Strategy 2: Do Lasso for all
# Libraries
library(pacman)
p_load(tidyverse, tidymodels, skimr, glmnet, kknn)
library(tidymodels)
library(openxlsx)
# Data
merge_final <- read.csv("Data/Merging/merge_last.csv")
#splitting data
merge_final_model <- merge_final %>%
select(-X.1, -X, -state_abb, -missing_eth, -missing_corn_prod, -missing_corn_prices, -X.2, -ratio_e85)
merge_final_model$year <- as.character(merge_final_model$year)
set.seed(42)
e85_split = merge_final_model |> group_initial_split(group=state, prop = 0.8)
e85_train = e85_split |> training()
e85_test  = e85_split |> testing()
e85_split
#recipe
e85_recipe =
# Define the recipe: Rating predicted by all other vars in housing_train
recipe(e85 ~ ., data = e85_train) |>
update_role(state, new_role = "ID") |>
# Create log terms for numeric predictors
step_log(total, population) |>
# Normalize vars
step_normalize(all_numeric_predictors()) |>
#dummy for years
step_dummy(year) |>
#interactions
step_interact(~all_predictors():all_predictors()) |>
# Remove predictors with near-zero variance (improves stability)
step_zv(all_numeric_predictors()) |>
# Remove very collinear vars
step_corr(all_numeric_predictors(), method = "spearman",threshold = 0.9)
system.time({
e85_clean <- e85_recipe |> prep() |> juice()
})
#model
model_lasso =
linear_reg(penalty = tune(), mixture = 1) |>
set_engine("glmnet")
#setting lambdas
#Values range from 10^-7 because from iterative process it seems that lower alphas perform better
lambdas = 10 ^ seq(from = -3, to = 3, length = 1e3)
e85_cv = e85_train |> group_vfold_cv(group=state, v = 5)
#defining workflow
workflow_lasso = workflow() |>
add_model(model_lasso) |>
add_recipe(e85_recipe)
#tuning for a Lasso regression model
lasso_cv = workflow_lasso |>
tune_grid(
e85_cv,
grid = data.frame(penalty = lambdas),
metrics = metric_set(yardstick::rmse)
)
lasso_cv |> show_best()
final_lasso =
workflow_lasso |>
finalize_workflow(select_best(lasso_cv, metric = "rmse"))
final_lasso
#graph
autoplot(lasso_cv, metric="rmse")
#on test data
final_fit_lasso = final_lasso |> fit(data=e85_train) |> last_fit(split=e85_split)
metrics= final_fit_lasso |> collect_metrics()
#looking at coefficients
coefs = final_fit_lasso |>
extract_fit_parsnip() |>
tidy()
#getting non-zero coefs
coefs_nonzero = coefs |>
filter(estimate > 0) |>
arrange(desc(estimate))
#graph
# Predicting values on the test set
lasso_fit <- final_lasso %>%
fit(data = e85_test)
predicted_values <- predict(lasso_fit, new_data = e85_test)
# Plotting predicted vs actual values
library(ggplot2)
predicted_values$actuals=e85_test$e85
plot1=ggplot(data=predicted_values, aes(x = actuals, y = .pred)) +
geom_point() +
geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
labs(title = "Predicted vs Actual Values",
x = "Actual #E85 stations",
y = "Predicted #E85 stations") +
theme_minimal()
ggsave("Outputs/Econometric_Analysis/predicted_actual.png", plot1, width = 8, height = 6, units = "in", dpi = 300)
#saving non-zero coefs
library(openxlsx)
write.xlsx(coefs_nonzero, file = "Outputs/Econometric_Analysis/coefs_nonzero.xlsx", rowNames = FALSE)
